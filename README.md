# n_grams_document_classificaton

Use https://drive.google.com/open?id=17zqtE2KlRtwKb2leiWA9b81g7U5UlQHh to load tokenizations folder. Github would not let me upload it.
