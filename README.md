# n_grams_document_classificaton

Use https://drive.google.com/open?id=17zqtE2KlRtwKb2leiWA9b81g7U5UlQHh to load tokenizations folder. Github would not let me upload it.
Figures - contains plots generated by experiments
Results - contains csvs generated by experiments
Train.csv - training set used
Validation.csv - validation set used
aclImdb - IMDb movie database
